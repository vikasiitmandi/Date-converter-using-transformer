{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "emg3z8nKttdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def find_last_checkpoint(checkpoint_dir):\n",
        "    epochs = []\n",
        "    for name in os.listdir(checkpoint_dir):\n",
        "        if os.path.splitext(name)[-1] == '.pth':\n",
        "            epochs += [int(name.strip('ckpt_epoch_.pth'))]\n",
        "    if len(epochs) == 0:\n",
        "        raise IOError('no checkpoint found in {}'.format(checkpoint_dir))\n",
        "    return max(epochs)\n",
        "\n",
        "def save_checkpoint(checkpoint_dir, epoch, model, optimizer=None):\n",
        "    checkpoint = {}\n",
        "    checkpoint['epoch'] = epoch\n",
        "\n",
        "    if isinstance(model, torch.nn.DataParallel):\n",
        "        model_state_dict = model.module.state_dict()\n",
        "    else:\n",
        "        model_state_dict = model.state_dict()\n",
        "    checkpoint['model'] = model_state_dict\n",
        "\n",
        "    if optimizer is not None:\n",
        "        optimizer_state_dict = optimizer.state_dict()\n",
        "        # for k, v in optimizer_state_dict.items():\n",
        "        #     print(k, type(v))\n",
        "        # optimizer_state_dict = rename_dict_key(optimizer_state_dict)\n",
        "        checkpoint['optimizer'] = optimizer_state_dict\n",
        "    else:\n",
        "        checkpoint['optimizer'] = None\n",
        "\n",
        "    torch.save(checkpoint, os.path.join(checkpoint_dir, 'ckpt_epoch_%02d.pth'% epoch))\n",
        "\n",
        "def load_checkpoint(checkpoint_dir, epoch=-1):\n",
        "    if epoch == -1:\n",
        "        epoch = find_last_checkpoint(checkpoint_dir)\n",
        "    checkpoint_name = 'ckpt_epoch_%02d.pth'% epoch\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n",
        "    ckpt = torch.load(checkpoint_path, map_location='cpu')\n",
        "    return ckpt\n",
        "\n",
        "def save_model(checkpoint_dir, epoch, model):\n",
        "    save_checkpoint(checkpoint_dir, epoch, model, optimizer=None)\n",
        "\n",
        "def load_model(checkpoint_dir, epoch, model):\n",
        "    try:\n",
        "        ckpt = load_checkpoint(checkpoint_dir, epoch)\n",
        "        model_state_dict = ckpt['model']\n",
        "\n",
        "        if isinstance(model, torch.nn.DataParallel):\n",
        "            model.module.load_state_dict(model_state_dict)\n",
        "        # elif isinstance(model, torchDDP):\n",
        "        #     model.module.load_state_dict(model_state_dict)\n",
        "        # elif isinstance(model, apexDDP):\n",
        "        #     model.module.load_state_dict(model_state_dict)\n",
        "        else:\n",
        "            model.load_state_dict(model_state_dict)\n",
        "    except Exception as e:\n",
        "        print('failed to load model, {}'.format(e))\n",
        "    return model\n",
        "\n",
        "def load_optimizer(checkpoint_dir, epoch, optimizer):\n",
        "    try:\n",
        "        ckpt = load_checkpoint(checkpoint_dir, epoch)\n",
        "        optimizer_state_dict = ckpt['optimizer']\n",
        "        optimizer.load_state_dict(optimizer_state_dict)\n",
        "    except Exception as e:\n",
        "        print('failed to load optimizer, {}'.format(e))\n",
        "    return optimizer"
      ],
      "metadata": {
        "id": "nm4reV5Mtqmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model"
      ],
      "metadata": {
        "id": "9RgGGJzVtxy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(torch.nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, n_position=50):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        self.linear = torch.nn.Linear(in_features=in_dim, out_features=out_dim) #word embedding\n",
        "        # Not a parameter\n",
        "        self.register_buffer('pos_table', self._get_sinusoid_encoding_table(n_position, out_dim))\n",
        "\n",
        "    def _get_sinusoid_encoding_table(self, n_position, out_dim):\n",
        "        ''' Sinusoid position encoding table '''\n",
        "        # TODO: make it with torch instead of numpy\n",
        "\n",
        "        def get_position_angle_vec(position):\n",
        "            return [position / np.power(10000, 2 * (hid_j // 2) / out_dim) for hid_j in range(out_dim)]\n",
        "\n",
        "        sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n",
        "        sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
        "        sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
        "\n",
        "        return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        return x + self.pos_table[:, :x.size(1)].clone().detach()\n",
        "\n",
        "class ScaledDotProductAttention(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        d_k = q.shape[-1]\n",
        "\n",
        "        scores = torch.matmul(q / (d_k ** 0.5), k.transpose(2, 3)) #(N, n_head, T, T)\n",
        "        if mask is not None:\n",
        "            # print(mask.unsqueeze(0).unsqueeze(0).shape, scores.shape)\n",
        "            scores = scores.masked_fill(mask.unsqueeze(0).unsqueeze(0)==0, -1e9)\n",
        "        scores = torch.nn.Softmax(dim=-1)(scores) #(N, n_head, T, T)\n",
        "        # print(scores.shape, scores[2, 1, 0, :].sum())\n",
        "        # print(scores[0, 0])\n",
        "\n",
        "        output = torch.matmul(scores, v) #(N, n_head, T, out_dim)\n",
        "        # print(output.shape)\n",
        "        return output, scores\n",
        "\n",
        "class MultiHeadAttention(torch.nn.Module):\n",
        "    def __init__(self, n_head, in_dim, out_dim):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.out_dim = out_dim\n",
        "\n",
        "        self.linear_q = torch.nn.Linear(in_features=in_dim, out_features=n_head*out_dim)\n",
        "        self.linear_k = torch.nn.Linear(in_features=in_dim, out_features=n_head*out_dim)\n",
        "        self.linear_v = torch.nn.Linear(in_features=in_dim, out_features=n_head*out_dim)\n",
        "\n",
        "        self.scaled_dot_production_attention = ScaledDotProductAttention()\n",
        "        self.linear = torch.nn.Linear(in_features=n_head*out_dim, out_features=out_dim)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "\n",
        "\n",
        "        batch_size, len_q, len_kv = q.shape[0], q.shape[1], k.shape[1]\n",
        "\n",
        "        q = self.linear_q(q).view(batch_size, len_q, self.n_head, self.out_dim) #(N, T, in_dim) --> (N, T, n_head * out_dim) --> (N, T, n_head, out_dim)\n",
        "        k = self.linear_k(k).view(batch_size, len_kv, self.n_head, self.out_dim)\n",
        "        v = self.linear_v(v).view(batch_size, len_kv, self.n_head, self.out_dim)\n",
        "\n",
        "\n",
        "        q = q.transpose(1, 2) #(N, T, n_head, out_dim) --> (N, n_head, T, out_dim)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "        # print(q.shape, k.shape, v.shape)\n",
        "\n",
        "        output, scores = self.scaled_dot_production_attention(q, k, v, mask=mask)\n",
        "\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, len_q, -1)\n",
        "        # print(output.shape)\n",
        "\n",
        "        output = self.linear(output) #(N, T, n_head * out_dim) --> (N, T, out_dim)\n",
        "        # print(output.shape)\n",
        "\n",
        "        return output, scores\n",
        "\n",
        "class PositionWiseFeedForward(torch.nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.linear_1 = torch.nn.Linear(in_features=in_dim, out_features=hidden_dim)\n",
        "        self.linear_2 = torch.nn.Linear(in_features=hidden_dim, out_features=in_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear_1(x)\n",
        "        x = torch.nn.ReLU()(x)\n",
        "        x = self.linear_2(x)\n",
        "        return x\n",
        "\n",
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, n_head, in_dim, out_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.position_enc = PositionalEncoding(in_dim, out_dim)\n",
        "\n",
        "        self.multi_head_attention_1 = MultiHeadAttention(n_head=n_head, in_dim=out_dim, out_dim=out_dim)\n",
        "        self.layer_norm_1_1 = torch.nn.LayerNorm(out_dim)\n",
        "\n",
        "        self.position_wise_feed_forward_1 = PositionWiseFeedForward(out_dim, hidden_dim=128)\n",
        "        self.layer_norm_1_2 = torch.nn.LayerNorm(out_dim)\n",
        "\n",
        "        self.scores_for_paint = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        qkv = self.position_enc(x) #(N, T, 37) --> (N, T, 64)\n",
        "\n",
        "        residual = qkv\n",
        "        outputs, scores = self.multi_head_attention_1(qkv, qkv, qkv)\n",
        "        self.scores_for_paint = scores.detach().cpu().numpy()\n",
        "        outputs = self.layer_norm_1_1(outputs + residual) #Add & Norm\n",
        "        # print(outputs.shape)\n",
        "\n",
        "        residual = outputs\n",
        "        outputs = self.position_wise_feed_forward_1(outputs)\n",
        "        outputs = self.layer_norm_1_2(outputs + residual) #Add & Norm\n",
        "        # print(outputs.shape)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "def get_subsequent_mask(seq):\n",
        "    seq_len = seq.shape[1]\n",
        "    ones = torch.ones((seq_len, seq_len), dtype=torch.int, device=seq.device)\n",
        "    mask = 1 - torch.triu(ones, diagonal=1)\n",
        "    # print(mask)\n",
        "    return mask\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, n_head, in_dim, out_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.position_enc = PositionalEncoding(in_dim, out_dim)\n",
        "\n",
        "        self.multi_head_attention_1_1 = MultiHeadAttention(n_head=n_head, in_dim=out_dim, out_dim=out_dim)\n",
        "        self.layer_norm_1_1 = torch.nn.LayerNorm(out_dim)\n",
        "\n",
        "        self.multi_head_attention_1_2 = MultiHeadAttention(n_head=n_head, in_dim=out_dim, out_dim=out_dim)\n",
        "        self.layer_norm_1_2 = torch.nn.LayerNorm(out_dim)\n",
        "\n",
        "        self.position_wise_feed_forward_1 = PositionWiseFeedForward(out_dim, hidden_dim=128)\n",
        "        self.layer_norm_1_3 = torch.nn.LayerNorm(out_dim)\n",
        "\n",
        "        self.scores_for_paint = None\n",
        "\n",
        "    def forward(self, enc_outputs, target):\n",
        "        qkv = self.position_enc(target)\n",
        "\n",
        "        residual = qkv\n",
        "        outputs, scores = self.multi_head_attention_1_1(qkv, qkv, qkv, mask=get_subsequent_mask(target))\n",
        "        outputs = self.layer_norm_1_1(outputs + residual)\n",
        "        # print(outputs.shape)\n",
        "\n",
        "        residual = outputs\n",
        "        outputs, scores = self.multi_head_attention_1_2(outputs, enc_outputs, enc_outputs)\n",
        "        self.scores_for_paint = scores.detach().cpu().numpy()\n",
        "        outputs = self.layer_norm_1_2(outputs + residual)\n",
        "\n",
        "        residual = outputs\n",
        "        outputs = self.position_wise_feed_forward_1(outputs)\n",
        "        outputs = self.layer_norm_1_3(outputs + residual)\n",
        "        # print(outputs.shape)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class Transformer(torch.nn.Module):\n",
        "    def __init__(self, n_head):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(n_head, in_dim=37, out_dim=64)\n",
        "        self.decoder = Decoder(n_head, in_dim=12, out_dim=64)\n",
        "        self.linear = torch.nn.Linear(in_features=64, out_features=12)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        enc_outputs = self.encoder(x)\n",
        "        outputs = self.decoder(enc_outputs, y)\n",
        "        outputs = self.linear(outputs)\n",
        "        # print(outputs.shape)\n",
        "        outputs = torch.nn.Softmax(dim=-1)(outputs)\n",
        "        # print(outputs.shape)\n",
        "        return outputs\n",
        "\n",
        "    def size(self):\n",
        "        size = sum([p.numel() for p in self.parameters()])\n",
        "        print('%.2fKB' % (size * 4 / 1024))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = Transformer(n_head=4)\n",
        "    model.size()\n",
        "\n",
        "    batch_x = torch.randn(16, 10, 37) #(N, T, in_dim)\n",
        "    batch_y = torch.randn(16, 7, 12) #(N, T, in_dim)\n",
        "    # print(batch_x.shape)\n",
        "\n",
        "    pred = model(batch_x, batch_y)\n",
        "    print(pred.shape)\n",
        "    print(pred[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKJdAbEft1NO",
        "outputId": "5ec4fb3d-c2b7-419e-a431-44bf9d80eaa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "925.55KB\n",
            "torch.Size([16, 7, 12])\n",
            "tensor([0.0984, 0.0704, 0.0451, 0.0492, 0.0335, 0.0345, 0.1726, 0.0286, 0.0791,\n",
            "        0.0732, 0.2643, 0.0512], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## save_load"
      ],
      "metadata": {
        "id": "xgcoP6AvuDAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def find_last_checkpoint(checkpoint_dir):\n",
        "    epochs = []\n",
        "    for name in os.listdir(checkpoint_dir):\n",
        "        if os.path.splitext(name)[-1] == '.pth':\n",
        "            epochs += [int(name.strip('ckpt_epoch_.pth'))]\n",
        "    if len(epochs) == 0:\n",
        "        raise IOError('no checkpoint found in {}'.format(checkpoint_dir))\n",
        "    return max(epochs)\n",
        "\n",
        "def save_checkpoint(checkpoint_dir, epoch, model, optimizer=None):\n",
        "    checkpoint = {}\n",
        "    checkpoint['epoch'] = epoch\n",
        "\n",
        "    if isinstance(model, torch.nn.DataParallel):\n",
        "        model_state_dict = model.module.state_dict()\n",
        "    else:\n",
        "        model_state_dict = model.state_dict()\n",
        "    checkpoint['model'] = model_state_dict\n",
        "\n",
        "    if optimizer is not None:\n",
        "        optimizer_state_dict = optimizer.state_dict()\n",
        "        # for k, v in optimizer_state_dict.items():\n",
        "        #     print(k, type(v))\n",
        "        # optimizer_state_dict = rename_dict_key(optimizer_state_dict)\n",
        "        checkpoint['optimizer'] = optimizer_state_dict\n",
        "    else:\n",
        "        checkpoint['optimizer'] = None\n",
        "\n",
        "    torch.save(checkpoint, os.path.join(checkpoint_dir, 'ckpt_epoch_%02d.pth'% epoch))\n",
        "\n",
        "def load_checkpoint(checkpoint_dir, epoch=-1):\n",
        "    if epoch == -1:\n",
        "        epoch = find_last_checkpoint(checkpoint_dir)\n",
        "    checkpoint_name = 'ckpt_epoch_%02d.pth'% epoch\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n",
        "    ckpt = torch.load(checkpoint_path, map_location='cpu')\n",
        "    return ckpt\n",
        "\n",
        "def save_model(checkpoint_dir, epoch, model):\n",
        "    save_checkpoint(checkpoint_dir, epoch, model, optimizer=None)\n",
        "\n",
        "def load_model(checkpoint_dir, epoch, model):\n",
        "    try:\n",
        "        ckpt = load_checkpoint(checkpoint_dir, epoch)\n",
        "        model_state_dict = ckpt['model']\n",
        "\n",
        "        if isinstance(model, torch.nn.DataParallel):\n",
        "            model.module.load_state_dict(model_state_dict)\n",
        "        # elif isinstance(model, torchDDP):\n",
        "        #     model.module.load_state_dict(model_state_dict)\n",
        "        # elif isinstance(model, apexDDP):\n",
        "        #     model.module.load_state_dict(model_state_dict)\n",
        "        else:\n",
        "            model.load_state_dict(model_state_dict)\n",
        "    except Exception as e:\n",
        "        print('failed to load model, {}'.format(e))\n",
        "    return model\n",
        "\n",
        "def load_optimizer(checkpoint_dir, epoch, optimizer):\n",
        "    try:\n",
        "        ckpt = load_checkpoint(checkpoint_dir, epoch)\n",
        "        optimizer_state_dict = ckpt['optimizer']\n",
        "        optimizer.load_state_dict(optimizer_state_dict)\n",
        "    except Exception as e:\n",
        "        print('failed to load optimizer, {}'.format(e))\n",
        "    return optimizer"
      ],
      "metadata": {
        "id": "FHZ3UI1JuGXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from functools import partial\n",
        "!pip install faker\n",
        "#from dataset import Dataset, transform, collate_fn\n",
        "#from model import Transformer\n",
        "#import save_load as sl\n",
        "\n",
        "def calc_accuracy(pred, answer):\n",
        "    # print(pred)\n",
        "    pred = np.argmax(pred, axis=2)\n",
        "    answer = np.argmax(answer, axis=2)\n",
        "    # print(pred.shape, answer.shape)\n",
        "    correct = (pred == answer).astype(np.int)\n",
        "    accuracy = correct.sum() / (pred.shape[0] * pred.shape[1])\n",
        "    # print(accuracy)\n",
        "    return accuracy\n",
        "\n",
        "def train(model, loss_fn, optimizer, dataloader, epoch, use_gpu=False):\n",
        "    pbar = tqdm(total=len(dataloader), bar_format='{l_bar}{r_bar}', dynamic_ncols=True)\n",
        "    pbar.set_description(f'Epoch %d' % epoch)\n",
        "\n",
        "    for step, (batch_x, batch_y, _) in enumerate(dataloader):\n",
        "        if use_gpu:\n",
        "            batch_x = batch_x.cuda()\n",
        "            batch_y = batch_y.cuda()\n",
        "        pred = model(batch_x, batch_y[:, :-1, :])\n",
        "        accuracy = calc_accuracy(pred.detach().cpu().numpy(), batch_y[:, 1:, :].detach().cpu().numpy())\n",
        "        # loss = loss_fn(pred.transpose(1, 2), batch_y)\n",
        "        loss = loss_fn(pred, batch_y[:, 1:, :])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pbar.set_postfix(**{'loss':loss.detach().cpu().item(), 'accuracy':accuracy})\n",
        "        pbar.update()\n",
        "    sl.save_checkpoint('/content/sample_data/checkpoint', epoch, model, optimizer)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "def main(gpu_id=None):\n",
        "    dataset = Dataset(transform=transform, n_datas=10000)\n",
        "    pad_vec = np.zeros(len(dataset.human_vocab))\n",
        "    pad_vec[dataset.human_vocab['<pad>']] = 1\n",
        "    dataloader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                            batch_size=6,\n",
        "                                            shuffle=True,\n",
        "                                            num_workers=6,\n",
        "                                            collate_fn=partial(collate_fn, pad_vec))\n",
        "\n",
        "    model = Transformer(n_head=2)\n",
        "    if gpu_id is not None:\n",
        "        print('use gpu')\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_id\n",
        "        n_gpus = torch.cuda.device_count()\n",
        "        # print('use %d gpu [%s]' % (n_gpus, gpu_id))\n",
        "        model = model.cuda()\n",
        "        # model = torch.nn.DataParallel(model, device_ids=[i for i in range(n_gpus)])\n",
        "    # loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "    model = sl.load_model('/content/sample_data/checkpoint', -1, model)\n",
        "    optimizer = sl.load_optimizer('/content/sample_data/checkpoint', -1, optimizer)\n",
        "\n",
        "    try:\n",
        "        trained_epoch = sl.find_last_checkpoint('/content/sample_data/checkpoint')\n",
        "        print('train form epoch %d' % (trained_epoch + 1))\n",
        "    except Exception as e:\n",
        "        print('train from the very begining, {}'.format(e))\n",
        "        trained_epoch = -1\n",
        "    for epoch in range(trained_epoch+1, 20):\n",
        "        train(model, loss_fn, optimizer, dataloader, epoch, use_gpu=True if gpu_id is not None else False)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if len(sys.argv) == 1:\n",
        "        main(gpu_id=None)\n",
        "    else:\n",
        "        main(gpu_id='0')"
      ],
      "metadata": {
        "id": "eB0xyPIUUmRr",
        "outputId": "f7446aef-eae6-4c80-cf8f-2af731d64b51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faker in /usr/local/lib/python3.10/dist-packages (25.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/10000 [00:00<?, ?it/s]\u001b[A\n",
            " 30%|██▉       | 2964/10000 [00:00<00:00, 29592.69it/s]\u001b[A\n",
            " 59%|█████▉    | 5924/10000 [00:00<00:00, 28092.19it/s]\u001b[A\n",
            "100%|██████████| 10000/10000 [00:00<00:00, 26637.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "use gpu\n",
            "train form epoch 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(model, x, y0):\n",
        "    x_tensor = torch.from_numpy(x).unsqueeze(0)\n",
        "    enc_output = model.encoder(x_tensor)\n",
        "    y0_tensor = torch.from_numpy(y0).unsqueeze(0).unsqueeze(0)\n",
        "    y_tensor = y0_tensor.clone()\n",
        "\n",
        "    for step in range(14):\n",
        "        dec_output = model.decoder(enc_output, y_tensor)\n",
        "        pred = model.linear(dec_output)\n",
        "        pred = torch.nn.Softmax(dim=-1)(pred)\n",
        "\n",
        "        y_tensor = torch.cat((y0_tensor, pred), dim=1)\n",
        "        # print(y_tensor.shape)\n",
        "    return y_tensor.squeeze(0)\n",
        "\n",
        "def paint_score(score, human_readable, pred):\n",
        "    '''\n",
        "    score: (n_head, T_dec, T_enc)\n",
        "    human_readabe: string, length is T_enc\n",
        "    pred: string, length is T_dec\n",
        "    '''\n",
        "    # print(score.shape, len(pred), len(human_readable))\n",
        "    # print(pred, human_readable)\n",
        "\n",
        "    n_head = score.shape[0]\n",
        "\n",
        "    f = plt.figure(figsize=(8, 8.5))\n",
        "    for i in range(n_head):\n",
        "        ax = f.add_subplot(n_head, 1, i+1)\n",
        "        i = ax.imshow(score[i, :10, :], interpolation='nearest', cmap='Blues')\n",
        "\n",
        "        ax.set_xticks(range(min(30, len(human_readable))))\n",
        "        ax.set_xticklabels(human_readable[:30], rotation=0)\n",
        "\n",
        "        #ax.set_xticks(range(30))\n",
        "        #ax.set_xticklabels(human_readable[:30], rotation=0)\n",
        "\n",
        "        ax.set_yticks(range(10))\n",
        "        ax.set_yticklabels(pred[:10], rotation=0)\n",
        "\n",
        "    plt.savefig('./attention.png')\n",
        "\n",
        "def main():\n",
        "    dataset = Dataset(transform=transform, n_datas=10000, seed=None)\n",
        "    model = Transformer(n_head=2)\n",
        "    try:\n",
        "        trained_epoch = sl.find_last_checkpoint('/content/sample_data/checkpoint')\n",
        "        print('load model %d' % (trained_epoch))\n",
        "    except Exception as e:\n",
        "        print('no trained model found, {}'.format(e))\n",
        "        return\n",
        "    model = sl.load_model('/content/sample_data/checkpoint', -1, model)\n",
        "    model.eval()\n",
        "\n",
        "    x, y, extra = dataset.__getitem__(0)\n",
        "    # print(x.shape, y.shape)\n",
        "    # pred = model(torch.from_numpy(x).unsqueeze(0), torch.from_numpy(y).unsqueeze(0)).squeeze()\n",
        "    pred = translate(model, x, y[0])\n",
        "    # print(pred.shape)\n",
        "    pred = np.argmax(pred.detach().numpy(), axis=1)[1:]\n",
        "    # print(extra['machine_readable'])\n",
        "    pred = [dataset.inv_machine_vocab[p] for p in pred]\n",
        "    pred_str = ''.join(pred)\n",
        "    human_readable = extra['human_readable']\n",
        "    machine_readable = extra['machine_readable']\n",
        "    print('[%s] --> [%s], answer: [%s]' % (human_readable, pred_str, list(machine_readable)))\n",
        "\n",
        "    dec_scores = model.decoder.scores_for_paint\n",
        "    # print(dec_scores.shape)\n",
        "    paint_score(dec_scores[0], human_readable, pred)\n",
        "\n",
        "    # print(len(model.scores_for_paint), model.scores_for_paint[0].shape)\n",
        "    # scores = np.array(model.scores_for_paint)\n",
        "    # print(np.argmax(scores, axis=1))\n",
        "\n",
        "    # f = plt.figure(figsize=(8, 8.5))\n",
        "    # ax = f.add_subplot(1, 1, 1)\n",
        "    # i = ax.imshow(scores, interpolation='nearest', cmap='Blues')\n",
        "\n",
        "    # ax.set_xticks(range(30))\n",
        "    # ax.set_xticklabels(human_readable[:30], rotation=0)\n",
        "\n",
        "    # ax.set_yticks(range(10))\n",
        "    # ax.set_yticklabels(machine_readable[:10], rotation=0)\n",
        "\n",
        "    # plt.savefig('./attention.png')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 891
        },
        "id": "NOiZPAIEpb4x",
        "outputId": "34d6dea0-ce87-4f2f-a8bb-50ce9d15c8c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/10000 [00:00<?, ?it/s]\u001b[A\n",
            " 19%|█▉        | 1922/10000 [00:00<00:00, 19219.06it/s]\u001b[A\n",
            " 38%|███▊      | 3844/10000 [00:00<00:00, 18002.85it/s]\u001b[A\n",
            " 57%|█████▋    | 5663/10000 [00:00<00:00, 18081.76it/s]\u001b[A\n",
            " 75%|███████▍  | 7475/10000 [00:00<00:00, 17928.19it/s]\u001b[A\n",
            "100%|██████████| 10000/10000 [00:00<00:00, 17866.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load model 19\n",
            "[25 apr 2013] --> [2013-04-25<pad><pad><pad><pad>], answer: [['2', '0', '1', '3', '-', '0', '4', '-', '2', '5']]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x850 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}